# -*- coding: utf-8 -*-
"""Capstone_ChatBot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1msRlGtwyJa7QUPOMLQxOHCAD0VlsS_Cs
"""

from google.colab import drive
drive.mount('/content/drive')

csv_file="/content/drive/MyDrive/Colab Notebooks/Data Set - industrial_safety_and_health_database_with_accidents_description.csv"

pip install contractions

pip install autocorrect

pip install nlpaug

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import os
import re
import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
from nltk.corpus import stopwords
stop = stopwords.words('english')
import pandas as pd
import contractions
from autocorrect import Speller
spell = Speller(lang='en')


from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from keras.utils import np_utils
from keras.regularizers import l1, l2, l1_l2
from keras.constraints import unit_norm

import pickle

from keras.layers.merge import Concatenate

import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

import nlpaug.augmenter.word as naw

from tqdm import tqdm

from tensorflow.keras.layers import Input, Dense, LSTM, Concatenate, Conv1D
from sklearn import metrics

df = pd.read_csv(csv_file)
df.sample(5)

"""Data Cleaning & Preprocessing"""

df.isna().any()

"""Shape of the data"""

df.shape

"""Datatype of each feature"""

df.info()

# Check duplicates in a data frame
df.duplicated().sum()

# View the duplicate records
duplicates = df.duplicated()

df[duplicates]

"""There are 7 duplicates in the dataset as below

We can remove the duplicate data as it is already a part of the dataset

Drop Duplicates
"""

# Delete duplicate rows
df.drop_duplicates(inplace=True)

# shaape of the data
df.shape

def preprocess_inputs(df):
  df= df.copy()

  #drop Unnamed column
  df.drop("Unnamed: 0", axis=1, inplace=True)

  #rename Columns- Data,Countries,Genre,Employee or Third Party
  df.rename(columns={'Data':'Date', 'Countries':'Country', 'Genre':'Gender', 'Employee or Third Party':'Employee type'}, inplace=True)

  # delete duplicated rows
  df.drop_duplicates(inplace=True)

  # remove spaces in columns name
  df.columns = df.columns.str.replace(' ','_')

  #replace roman numbers
  convert_roman = {'I': 1, 'II': 2, 'III': 3, 'IV': 4, 'V': 5, 'VI': 6}
  df.Accident_Level = df.Accident_Level.replace(convert_roman)
  df.Potential_Accident_Level = df.Potential_Accident_Level.replace(convert_roman)

  #df.Gender = df.Gender.replace({'Male': 1, 'Female': 0})


   # Adding Datetime Features such as year, Month and day
  df['Date'] = pd.to_datetime(df['Date'])
  df['Year'] = df['Date'].apply(lambda x : x.year)
  df['Month'] = df['Date'].apply(lambda x : x.month)
  df['Day'] = df['Date'].apply(lambda x : x.day)
  df['Weekday'] = df['Date'].apply(lambda x : x.day_name())
  df['WeekofYear'] = df['Date'].apply(lambda x : x.weekofyear)

  #Month variable into season
  df['Season'] = df['Month'].apply(month2seasons)
  
  return df

# function to create month variable into seasons
def month2seasons(x):
    if x in [9, 10, 11]:
        season = 'Spring'
    elif x in [12, 1, 2]:
        season = 'Summer'
    elif x in [3, 4, 5]:
        season = 'Autumn'
    elif x in [6, 7, 8]:
        season = 'Winter'
    return season

df = preprocess_inputs(df)

df.head()

df.isna().sum()

df.shape

pip install autocorrect

pip install contractions

"""NLP_PreProcessing"""

class PreProcessing:
    def __init__(self, get_lower_case=False,
                        remove_whitespaces=False,
                        remove_spl_char=False,
                        replace_contractions=False,
                        split_attached_words=False,
                        get_spell_check=False,
                        remove_stop_words=False):
        self.get_lower_case=get_lower_case
        self.remove_whitespaces=remove_whitespaces
        self.remove_spl_char=remove_spl_char
        self.replace_contractions=replace_contractions
        self.split_attached_words=split_attached_words
        self.get_spell_check=get_spell_check
        self.remove_stop_words=remove_stop_words

     #returns the lowercased strings
    def getLowerCase(self, text):
        return text.lower()

    #words that are shortened by dropping letters and replacing them by an apostrophe
    def replaceContractions(self, text):
        return contractions.fix(text)
    
    def splitAttachedWords(self, text):
        return " ".join([s for s in re.split("([A-Z][a-z]+[^A-Z]*)",text) if s])

    #spell check
    def getSpellCheck(self, text):
        return spell(text)
    
    #remove spaces or specified characters at the start and end of a string
    def removeWhiteSpace(self, text):
        text = re.sub('\W+',' ',text)
        return text.strip()
    
    #remove Special Characters
    def removeSpecialCharacters(self,text):
        text=re.sub('[^A-Za-z0-9]+', ' ', text)
        return text
        
    # remove stop words
    def removeStopWords(self,text):
        words=nltk.tokenize.word_tokenize(text)
        words_new=[i for i in words if i not in stop]
        return " ".join(words_new)
      
    
    def preprocess(self, text):

        if self.get_lower_case:
            text = self.getLowerCase(text)

        if self.replace_contractions:
            text = self.replaceContractions(text)

        if self.split_attached_words:
            text = self.splitAttachedWords(text)
            
        if self.get_spell_check:
            text = self.getSpellCheck(text)

        if self.remove_spl_char:
            text = self.removeSpecialCharacters(text)

        if self.remove_whitespaces:
            text = self.removeWhiteSpace(text)
            
        if self.remove_stop_words:
            text=self.removeStopWords(text)

        return text

pp = PreProcessing(get_lower_case=True,
remove_whitespaces=True,
remove_spl_char=True,
replace_contractions=True,
split_attached_words=True,
get_spell_check=True,
remove_stop_words=True)

df['Description_normalized']=df['Description'].apply(pp.preprocess)

df.isna().sum()

df.head(10)

df

"""Get the Length of each line and find the maximum length"""

# Get length of each line
df['line_length'] = df['Description_normalized'].str.len()

print('Minimum line length: {}'.format(df['line_length'].min()))
print('Maximum line length: {}'.format(df['line_length'].max()))
print('Line with maximum length: {}'.format(df[df['line_length'] == df['line_length'].max()]['Description_normalized'].values[0]))

# Get length of each line
df['nb_words'] = df['Description_normalized'].apply(lambda x: len(str(x).split(' ')))

print('Minimum number of words: {}'.format(df['nb_words'].min()))
print('Maximum number of words: {}'.format(df['nb_words'].max()))
print('Line with maximum number of words: {}'.format(df[df['nb_words'] == df['nb_words'].max()]['Description_normalized'].values[0]))

df.head(20)

df.isna().sum()

"""Feature Engineering"""

def feature_engineering(df):
  df= df.copy()

  # Create Industry DataFrame
  indus_df = pd.DataFrame()

  # Label encoding season
  df['Season'] = df['Season'].replace('Summer', 'aSummer').replace('Autumn', 'bAutumn').replace('Winter', 'cWinter').replace('Spring', 'dSpring')
  indus_df['Season'] = LabelEncoder().fit_transform(df['Season']).astype(np.int8)


  # Label encoding Weekday
  df['Weekday'] = df['Weekday'].replace('Monday', 'aMonday').replace('Tuesday', 'bTuesday').replace('Wednesday', 'cWednesday').replace('Thursday', 'dThursday').replace('Friday', 'eFriday').replace('Saturday', 'fSaturday').replace('Sunday', 'gSunday')
  indus_df['Weekday'] = LabelEncoder().fit_transform(df['Weekday']).astype(np.int8)


  # Label encoding Accident_Level
  indus_df['Accident_Level'] = LabelEncoder().fit_transform(df['Accident_Level']).astype(np.int8)


  # Label encoding Potential_Accident_Level
  indus_df['Potential_Accident_Level'] = LabelEncoder().fit_transform(df['Potential_Accident_Level']).astype(np.int8)


  # Dummy variables encoding
  Country_dummies = pd.get_dummies(df['Country'], columns=["Country"], drop_first=True)
  Local_dummies = pd.get_dummies(df['Local'], columns=["Local"], drop_first=True)
  Gender_dummies = pd.get_dummies(df['Gender'], columns=["Gender"], drop_first=True)
  IS_dummies = pd.get_dummies(df['Industry_Sector'], columns=['Industry_Sector'], prefix='IS', drop_first=True)
  EmpType_dummies = pd.get_dummies(df['Employee_type'], columns=['Employee_type'], prefix='EmpType', drop_first=True)
  CR_dummies = pd.get_dummies(df['Critical_Risk'], columns=['Critical_Risk'], prefix='CR', drop_first=True)

  # Merge the above dataframe with the original dataframe indus_df
  indus_df = indus_df.join(Country_dummies.reset_index(drop=True)).join(Local_dummies.reset_index(drop=True)).join(Gender_dummies.reset_index(drop=True)).join(IS_dummies.reset_index(drop=True)).join(EmpType_dummies.reset_index(drop=True)).join(CR_dummies.reset_index(drop=True))

  indus_df = df[['Year','Month','Day','WeekofYear']].reset_index(drop=True).join(indus_df.reset_index(drop=True))

  return indus_df

# Create Industry DataFrame
indus_df = pd.DataFrame()
indus_df = feature_engineering(df)

indus_df.head()

len(indus_df.columns)

# Check NaN values
np.any(np.isnan(indus_df))

indus_df

"""Design, train and test RNN or LSTM classifiers

model with Text inputs (accident description alone)
"""

# Select input and output features
X_text = df['Description_normalized']
y_text = df['Accident_Level']

X_text

# Encode labels in column 'Accident Level'.
from sklearn.preprocessing import LabelEncoder
y_text = LabelEncoder().fit_transform(y_text)

# pre-processing methods
from sklearn.model_selection import train_test_split

# Divide our data into testing and training sets:
X_text_train, X_text_test, y_text_train, y_text_test = train_test_split(X_text, y_text, test_size = 0.20, random_state = 1, stratify = y_text)

print('X_text_train shape : ({0})'.format(X_text_train.shape[0]))
print('y_text_train shape : ({0},)'.format(y_text_train.shape[0]))
print('X_text_test shape : ({0})'.format(X_text_test.shape[0]))
print('y_text_test shape : ({0},)'.format(y_text_test.shape[0]))

from keras.utils import np_utils
# Convert both the training and test labels into one-hot encoded vectors:
# Convert both the training and test labels into one-hot encoded vectors:
y_text_train = np_utils.to_categorical(y_text_train)
y_text_test = np_utils.to_categorical(y_text_test)

y_text_train

X_text_train[0]

pip install nlpaug

# Keras pre-processing
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# The first step in word embeddings is to convert the words into thier corresponding numeric indexes.
tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(X_text_train)

X_text_train = tokenizer.texts_to_sequences(X_text_train)
X_text_test = tokenizer.texts_to_sequences(X_text_test)

# Sentences can have different lengths, and therefore the sequences returned by the Tokenizer class also consist of variable lengths.
# We need to pad the our sequences using the max length.
vocab_size = len(tokenizer.word_index) + 1
print("vocab_size:", vocab_size)

maxlen = 100

X_text_train = pad_sequences(X_text_train, padding='post', maxlen=maxlen)
X_text_test = pad_sequences(X_text_test, padding='post', maxlen=maxlen)

X_text_train[0].size

embedding_size = 50
embeddings_dictionary = dict()

glove_file = open('/content/drive/MyDrive/Colab Notebooks/glove.6B.50d (3).txt', encoding="utf8")

for line in glove_file:
    records = line.split()
    word = records[0]
    vector_dimensions = np.asarray(records[1:], dtype='float32')
    embeddings_dictionary[word] = vector_dimensions

glove_file.close()

embedding_matrix = np.zeros((vocab_size, embedding_size))

for word, index in tokenizer.word_index.items():
    embedding_vector = embeddings_dictionary.get(word)
    if embedding_vector is not None:
        embedding_matrix[index] = embedding_vector

len(embeddings_dictionary.values())

print(embedding_matrix.shape)

from keras.layers import Input
from tensorflow.keras.layers import Flatten, Activation, Dense, LSTM, BatchNormalization, Embedding, Dropout, Flatten, Bidirectional, GlobalMaxPool1D
from keras.models import Model
from tensorflow.keras.optimizers import SGD
# Build a LSTM Neural Network
deep_inputs = Input(shape=(maxlen,))
embedding_layer = Embedding(vocab_size, embedding_size, weights=[embedding_matrix], trainable=False)(deep_inputs)

LSTM_Layer_1 = Bidirectional(LSTM(128, return_sequences = True))(embedding_layer)
max_pool_layer_1 = GlobalMaxPool1D()(LSTM_Layer_1)
drop_out_layer_1 = Dropout(0.5, input_shape = (256,))(max_pool_layer_1)
dense_layer_1 = Dense(128, activation = 'relu')(drop_out_layer_1)
drop_out_layer_2 = Dropout(0.5, input_shape = (128,))(dense_layer_1)
dense_layer_2 = Dense(64, activation = 'relu')(drop_out_layer_2)
drop_out_layer_3 = Dropout(0.5, input_shape = (64,))(dense_layer_2)
dense_layer_3 = Dense(32, activation = 'relu')(drop_out_layer_3)
drop_out_layer_4 = Dropout(0.5, input_shape = (32,))(dense_layer_3)
dense_layer_4 = Dense(10, activation = 'relu')(drop_out_layer_4)
drop_out_layer_5 = Dropout(0.5, input_shape = (10,))(dense_layer_4)
dense_layer_5 = Dense(5, activation='softmax')(drop_out_layer_5)
model_lstm_text_input = Model(inputs=deep_inputs, outputs=dense_layer_5)
opt = SGD(learning_rate=0.001, momentum=0.9)
model_lstm_text_input.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['acc'])

print(model_lstm_text_input.summary())

from keras.utils.vis_utils import plot_model
plot_model(model_lstm_text_input, to_file='model_plot1.png', show_shapes=True, show_dtype=True, show_layer_names=True)

import tensorflow as tf
tf.__version__

class Metrics(tf.keras.callbacks.Callback):

    def __init__(self, validation_data=()):
        super().__init__()
        self.validation_data = validation_data

    def on_train_begin(self, logs={}):
        self.val_f1s = []
        self.val_recalls = []
        self.val_precisions = []

    def on_epoch_end(self, epoch, logs={}):
        xVal, yVal, target_type = self.validation_data
        if target_type == 'multi_class':
          val_predict_classes = model.predict_classes(xVal, verbose=0) # Multiclass
        else:
          val_predict_classes = (np.asarray(self.model.predict(xVal))).round() # Multilabel
        
        
        val_targ = yVal

        _val_f1 = f1_score(val_targ, val_predict_classes, average='micro')
        _val_recall = recall_score(val_targ, val_predict_classes, average='micro')
        _val_precision = precision_score(val_targ, val_predict_classes, average='micro')
        self.val_f1s.append(_val_f1)
        self.val_recalls.append(_val_recall)
        self.val_precisions.append(_val_precision)
        #print("— train_f1: %f — train_precision: %f — train_recall %f" % (_val_f1, _val_precision, _val_recall))
        return

import tensorflow as tf
from keras.callbacks import ReduceLROnPlateau
from keras.callbacks import EarlyStopping
from sklearn import metrics
from sklearn import metrics
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, recall_score, precision_score, roc_auc_score
# Use earlystopping
# callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5, min_delta=0.001)
callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=7, min_delta=1E-3)
rlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.0001, patience=5, min_delta=1E-4)

target_type = 'multi_label'
metrics = Metrics(validation_data=(X_text_train, y_text_train, target_type))

# fit the keras model on the dataset
training_history = model_lstm_text_input.fit(X_text_train, y_text_train, epochs=100, batch_size=8, verbose=1, validation_data=(X_text_test, y_text_test), callbacks=[rlrp, metrics])

# evaluate the keras model
_, train_accuracy = model_lstm_text_input.evaluate(X_text_train, y_text_train, batch_size=8, verbose=0)
_, test_accuracy = model_lstm_text_input.evaluate(X_text_test, y_text_test, batch_size=8, verbose=0)

print('Train accuracy: %.2f' % (train_accuracy*100))
print('Test accuracy: %.2f' % (test_accuracy*100))

#get the accuracy, precision, recall, f1 score from model
def get_classification_metrics(model_lstm_text_input, X_test, y_test, target_type):
  
  # predict probabilities for test set
  yhat_probs = model_lstm_text_input.predict(X_test, verbose=0) # Multiclass

  # predict crisp classes for test set
  if target_type == 'multi_class':
    yhat_classes = model_lstm_text_input.predict_classes(X_test, verbose=0) # Multiclass
  else:
    yhat_classes = (np.asarray(model_lstm_text_input.predict(X_test))).round() # Multilabel

  # reduce to 1d array
  yhat_probs = yhat_probs[:, 0]

  # accuracy: (tp + tn) / (p + n)
  accuracy = accuracy_score(y_test, yhat_classes)

  # precision tp / (tp + fp)
  precision = precision_score(y_test, yhat_classes, average='micro')

  # recall: tp / (tp + fn)
  recall = recall_score(y_test, yhat_classes, average='micro')

  # f1: 2 tp / (2 tp + fp + fn)
  f1 = f1_score(y_test, yhat_classes, average='micro')

  return accuracy, precision, recall, f1

accuracy, precision, recall, f1 = get_classification_metrics(model_lstm_text_input, X_text_test, y_text_test, target_type)
print('Accuracy: %f' % accuracy)
print('Precision: %f' % precision)
print('Recall: %f' % recall)
print('F1 score: %f' % f1)

epochs = range(len(training_history.history['loss'])) # Get number of epochs

# plot loss learning curves
plt.plot(epochs, training_history.history['loss'], label = 'train')
plt.plot(epochs, training_history.history['val_loss'], label = 'test')
plt.legend(loc = 'upper right')
plt.title ('Training and validation loss')

"""The above one is good fit"""

# plot accuracy learning curves
plt.plot(epochs, training_history.history['acc'], label = 'train')
plt.plot(epochs, training_history.history['val_acc'], label = 'test')
plt.legend(loc = 'upper right')
plt.title ('Training and validation accuracy')

y_predti = model_lstm_text_input.predict(X_text_test)

y_pred_ti=np.argmax(y_predti, axis=1)
y_test_ci=np.argmax(y_text_test, axis=1)
cm_ti = confusion_matrix(y_test_ci, y_pred_ti)
print(cm_ti)

from sklearn import metrics
print('Report:' + '\n', metrics.classification_report(y_test_ci,y_pred_ti))
print('Cm:' + '\n', metrics.confusion_matrix(y_test_ci,y_pred_ti))

f = sns.heatmap(cm_ti, annot=True, fmt='d')

# serialize model to JSON
model_lstm_text_input_json = model_lstm_text_input.to_json()
with open("model_lstm_text_input.json", "w") as json_file:
    json_file.write(model_lstm_text_input_json)
    
# serialize weights to HDF5
model_lstm_text_input.save_weights("model_lstm_text_input.h5")
print("Saved model weights to disk")

# Save the model in h5 format 
model_lstm_text_input.save("model_lstm_text_input_finalized_keras.h5")
print("Saved model to disk")

#Pickle file
filename = 'model_lstm_text_input.pkl'
pickle.dump(model_lstm_text_input, open(filename, 'wb'))

"""Creating a Model with Categorical features Only"""

# Select input and output features
X_cat = indus_df[['Year','Month','Day','Country_02','Country_03']]
y_cat = df['Accident_Level']

# Encode labels in column 'Accident Level'.
y_cat = LabelEncoder().fit_transform(y_cat)

# Divide our data into testing and training sets:
X_cat_train, X_cat_test, y_cat_train, y_cat_test = train_test_split(X_cat, y_cat, test_size = 0.20, random_state = 1, stratify = y_cat)

print('X_cat_train shape : ({0})'.format(X_cat_train.shape[0]))
print('y_cat_train shape : ({0},)'.format(y_cat_train.shape[0]))
print('X_cat_test shape : ({0})'.format(X_cat_test.shape[0]))
print('y_cat_test shape : ({0},)'.format(y_cat_test.shape[0]))

# Convert both the training and test labels into one-hot encoded vectors:
y_cat_train = np_utils.to_categorical(y_cat_train)
y_cat_test = np_utils.to_categorical(y_cat_test)

X_cat_train.iloc[:]

# Variable transformation using StandardScaler
scaler_X = StandardScaler()#StandardScaler()
X_cat_train.iloc[:,:6] = scaler_X.fit_transform(X_cat_train.iloc[:,:6]) # Scaling only first 6 feautres

X_cat_test.iloc[:,:6] = scaler_X.fit_transform(X_cat_test.iloc[:,:6]) # Scaling only first 6 feautres

X_cat_train.shape[1]

param = 1e-4
input2 = Input(shape=(X_cat_train.shape[1],))
dense_layer_1 = Dense(10, input_dim=X_cat_train.shape[1], activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(param),kernel_constraint=unit_norm())(input2)
drop_out_layer_1 = Dropout(0.2)(dense_layer_1)
batch_norm_layer_1 = BatchNormalization()(drop_out_layer_1)

dense_layer_2 = Dense(10, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(param),kernel_constraint=unit_norm())(batch_norm_layer_1)
drop_out_layer_2 = Dropout(0.5)(dense_layer_2)
batch_norm_layer_2 = BatchNormalization()(drop_out_layer_2)

dense_layer_3 = Dense(5, activation='softmax', kernel_regularizer=l2(param), kernel_constraint=unit_norm())(batch_norm_layer_2)

model_Categorical_input = Model(inputs=input2, outputs=dense_layer_3)

opt = SGD(learning_rate=0.001, momentum=0.9)

model_Categorical_input.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['acc'])

print(model_Categorical_input.summary())

plot_model(model_Categorical_input, to_file='model_plot1.png', show_shapes=True, show_dtype=True, show_layer_names=True)

# Use earlystopping
callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=7, min_delta=1E-3)
rlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.0001, patience=5, min_delta=1E-4)

target_type = 'multi_label'
metrics = Metrics(validation_data=(X_cat_train, y_cat_train, target_type))

# fit the keras model on the dataset
training_history = model_Categorical_input.fit(X_cat_train, y_cat_train, epochs=100, batch_size=8, verbose=1, validation_data=(X_cat_test, y_cat_test), callbacks=[rlrp, metrics])

# evaluate the keras model
_, train_accuracy = model_Categorical_input.evaluate(X_cat_train, y_cat_train, batch_size=8, verbose=0)
_, test_accuracy = model_Categorical_input.evaluate(X_cat_test, y_cat_test, batch_size=8, verbose=0)

print('Train accuracy: %.2f' % (train_accuracy*100))
print('Test accuracy: %.2f' % (test_accuracy*100))

accuracy, precision, recall, f1 = get_classification_metrics(model_Categorical_input, X_cat_test, y_cat_test, target_type)
print('Accuracy: %f' % accuracy)
print('Precision: %f' % precision)
print('Recall: %f' % recall)
print('F1 score: %f' % f1)

epochs = range(len(training_history.history['loss'])) # Get number of epochs

# plot loss learning curves
plt.plot(epochs, training_history.history['loss'], label = 'train')
plt.plot(epochs, training_history.history['val_loss'], label = 'test')
plt.legend(loc = 'upper right')
plt.title ('Training and validation loss')

# plot accuracy learning curves
plt.plot(epochs, training_history.history['acc'], label = 'train')
plt.plot(epochs, training_history.history['val_acc'], label = 'test')
plt.legend(loc = 'upper right')
plt.title ('Training and validation accuracy')

y_predci = model_Categorical_input.predict(X_cat_test)

y_pred_ci=np.argmax(y_predci, axis=1)
y_test_ci=np.argmax(y_cat_test, axis=1)
cm_ci = confusion_matrix(y_test_ci, y_pred_ci)
print(cm_ci)

from sklearn import metrics
print('Report:' + '\n', metrics.classification_report(y_test_ci,y_pred_ci))
print('Cm:' + '\n', metrics.confusion_matrix(y_test_ci,y_pred_ci))

f = sns.heatmap(cm_ci, annot=True, fmt='d')

# serialize weights to HDF5
model_Categorical_input.save_weights("model_Categorical_input.h5")
print("Saved model weights to disk")

#Pickle file
filename = 'model_Categorical_input.pkl'
pickle.dump(model_Categorical_input, open(filename, 'wb'))

"""LSTM with multi input"""

embedding_size

vocab_size

inp_1 = Input(shape=(maxlen,))
inp_1 = Conv1D(64,3)(inp_1)
inp_1 = Dense(62, activation='relu')(inp_1)

inp_2 = Input(shape = (20,20))
inp_2 = LSTM(20, activation='relu')(inp_2)

concat_layer = Concatenate()([inp_1, inp_2])
dense_layer_3 = Dense(10, activation='relu')(concat_layer)
output = Dense(5, activation='softmax')(dense_layer_3)
# Model definition

model = Model(inputs=[inp_1, inp_2], outputs=output)

input_1 = Input(shape=(maxlen,))
embedding_layer   = Embedding(vocab_size, embedding_size, weights=[embedding_matrix], trainable=True)(input_1)
LSTM_Layer_1      = Bidirectional(LSTM(128, return_sequences = True))(embedding_layer)
max_pool_layer_1  = GlobalMaxPool1D()(LSTM_Layer_1)
drop_out_layer_1  = Dropout(0.5, input_shape = (256,))(max_pool_layer_1)
dense_layer_1     = Dense(128, activation = 'relu')(drop_out_layer_1)
drop_out_layer_2  = Dropout(0.5, input_shape = (128,))(dense_layer_1)
dense_layer_2     = Dense(64, activation = 'relu')(drop_out_layer_2)
drop_out_layer_3  = Dropout(0.5, input_shape = (64,))(dense_layer_2)

dense_layer_3     = Dense(32, activation = 'relu')(drop_out_layer_3)
drop_out_layer_4  = Dropout(0.5, input_shape = (32,))(dense_layer_3)

dense_layer_4     = Dense(10, activation = 'relu')(drop_out_layer_4)
drop_out_layer_5 = Dropout(0.5, input_shape = (10,))(dense_layer_4)

#submodel
param = 1e-4

input_2 = Input(shape=(X_cat_train.shape[1],))
dense_layer_5       = Dense(10, input_dim=X_cat_train.shape[1], activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(param),
                      kernel_constraint=unit_norm())(input_2)
drop_out_layer_6    = Dropout(0.2)(dense_layer_5)
batch_norm_layer_1  = BatchNormalization()(drop_out_layer_6)
dense_layer_6       = Dense(10, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(param), 
                            kernel_constraint=unit_norm())(batch_norm_layer_1)
drop_out_layer_7   = Dropout(0.5)(dense_layer_6)
batch_norm_layer_2 = BatchNormalization()(drop_out_layer_7)

concat_layer        = Concatenate()([drop_out_layer_5, batch_norm_layer_2])
dense_layer_7       = Dense(10, activation='relu')(concat_layer)
output  = Dense(5, activation='softmax')(dense_layer_7)
model_multiple_input   = Model(inputs=[input_1, input_2], outputs=output)

opt = SGD(learning_rate=0.001, momentum=0.9)
model_multiple_input.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['acc'])

embedding_matrix.shape

input_1 = Input(shape=(maxlen,))
embedding_layer   = Embedding(vocab_size, embedding_size, weights=[embedding_matrix], trainable=True)(input_1)
print(embedding_layer)
LSTM_Layer_1      = Bidirectional(LSTM(128, return_sequences = True))(embedding_layer)
print(LSTM_Layer_1)

dense_layer_1     = Conv1D(256,3, activation = 'relu')(LSTM_Layer_1)
print(dense_layer_1.shape)

input_1 = Input(shape=(maxlen,))
embedding_layer   = Embedding(vocab_size, embedding_size, weights=[embedding_matrix], trainable=True)(input_1)
print(embedding_layer)
LSTM_Layer_1      = Bidirectional(LSTM(128, return_sequences = True))(embedding_layer)
print(LSTM_Layer_1)
dense_layer_1     = Conv1D(256,3, activation = 'relu')(LSTM_Layer_1)

print(dense_layer_1.shape)
#submodel
param = 1e-4

input_2 = Input(shape=(None,X_cat_train.shape[1],))
print(input_2.shape)
dense_layer_5       = Dense(10, input_dim=X_cat_train.shape[1], activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(param),
                      kernel_constraint=unit_norm())(input_2)
print(dense_layer_5.shape)

input_1 = Input(shape=(maxlen,))
embedding_layer   = Embedding(vocab_size, embedding_size, weights=[embedding_matrix], trainable=True)(input_1)
print(embedding_layer)
LSTM_Layer_1      = Bidirectional(LSTM(128, return_sequences = True))(embedding_layer)
print(LSTM_Layer_1)
dense_layer_1     = Conv1D(256,3, activation = 'relu')(LSTM_Layer_1)


#submodel
param = 1e-4

input_2 = Input(shape=(None,X_cat_train.shape[1],))
dense_layer_5       = Dense(10, input_dim=X_cat_train.shape[1], activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(param),
                      kernel_constraint=unit_norm())(input_2)
drop_out_layer_6    = Dropout(0.2)(dense_layer_5)
batch_norm_layer_1  = BatchNormalization()(drop_out_layer_6)
dense_layer_6       = Dense(10, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(param), 
                            kernel_constraint=unit_norm())(batch_norm_layer_1)
drop_out_layer_7   = Dropout(0.5)(dense_layer_6)
batch_norm_layer_2 = BatchNormalization()(drop_out_layer_7)

concat_layer        = Concatenate()([dense_layer_1, batch_norm_layer_2])
dense_layer_7       = Dense(10, activation='relu')(concat_layer)
output  = Dense(5, activation='softmax')(dense_layer_7)
model_multiple_input   = Model(inputs=[input_1, input_2], outputs=output)

opt = SGD(learning_rate=0.001, momentum=0.9)
model_multiple_input.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['acc'])

print(model_multiple_input.summary())

plot_model(model_multiple_input, to_file='model_plot3.png', show_shapes=True, show_layer_names=True)

X_text_train.shape

y_cat_train.shape

#Use earlystopping
callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=7, min_delta=1E-3)
rlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.0001, patience=5, min_delta=1E-4)

target_type = 'multi_label'
metrics = Metrics(validation_data=([(None,X_text_train), (None,X_cat_train)], (None,y_cat_train), target_type))

# fit the keras model on the dataset
training_history = model_multiple_input.fit([(None,X_text_train, X_cat_train],y_cat_train, epochs=100, batch_size=8, verbose=1, validation_data=([(None,X_text_test),(None,X_cat_test)], y_cat_test), callbacks=[rlrp, metrics])

# evaluate the keras model
_, train_accuracy = model_multiple_input.evaluate([X_text_train, X_cat_train], y_cat_train, batch_size=8, verbose=0)
_, test_accuracy = model_multiple_input.evaluate([X_text_test, X_cat_test], y_cat_test, batch_size=8, verbose=0)

print('Train accuracy: %.2f' % (train_accuracy*100))
print('Test accuracy: %.2f' % (test_accuracy*100))

accuracy, precision, recall, f1 = get_classification_metrics(model_multiple_input, [X_text_test, X_cat_test], y_cat_test, target_type)
print('Accuracy: %f' % accuracy)
print('Precision: %f' % precision)
print('Recall: %f' % recall)
print('F1 score: %f' % f1)

epochs = range(len(training_history.history['loss'])) # Get number of epochs

# plot loss learning curves
plt.plot(epochs, training_history.history['loss'], label = 'train')
plt.plot(epochs, training_history.history['val_loss'], label = 'test')
plt.legend(loc = 'upper right')
plt.title ('Training and validation loss')

# plot accuracy learning curves
plt.plot(epochs, training_history.history['acc'], label = 'train')
plt.plot(epochs, training_history.history['val_acc'], label = 'test')
plt.legend(loc = 'upper right')
plt.title ('Training and validation accuracy')

# serialize weights to HDF5
model_Categorical_input.save_weights("model_multiple_input.h5")
print("Saved model weights to disk")

#Pickle file
filename = 'model_multiple_input.pkl'
pickle.dump(model_multiple_input, open(filename, 'wb'))

y_predmi = model_multiple_input.predict([X_text_test, X_cat_test])

y_pred_mi=np.argmax(y_predmi, axis=1)
y_test_mi=np.argmax(y_cat_test, axis=1)
cm_mi = confusion_matrix(y_test_mi, y_pred_mi)
print(cm_mi)

print('Report:' + '\n', metrics.classification_report(y_test_mi,y_pred_mi))
print('Cm:' + '\n', metrics.confusion_matrix(y_test_mi,y_pred_mi))

f = sns.heatmap(cm_mi, annot=True, fmt='d')

"""Data Augmentation"""

df["Accident_Level"].value_counts(normalize=True)

sns.countplot(x = 'Accident_Level', data = df)

df = df[['Description_normalized', 'Accident_Level']]

train,valid=train_test_split(df,test_size=0.15)

print('Shape of train',train.shape)
print("Shape of Validation ",valid.shape)

import nlpaug.augmenter.word as naw
aug = naw.SynonymAug()

train.head(5)

valid.head(5)

aug_syn = naw.SynonymAug(aug_src='wordnet')

text = 'The quick brown fox jumps over the lazy dog .'

print("Original:")
print(text)
print("Augmented Synonym Text:")
for ii in range(5):
    augmented_text = aug_syn.augment(text)
    print(augmented_text)

pip install tqdm

from sklearn.utils import shuffle

def augment_text(df,samples=300,pr=0.2):
    aug_syn.aug_p=pr
    new_text=[]
    
    ##dropping samples from validation
    df_n=df[df.Accident_Level!=1].reset_index(drop=True)

    ## data augmentation loop
    for i in tqdm(np.random.randint(0,len(df_n),samples)):
        
            text = df.iloc[i]['Description_normalized']
            augmented_text = aug_syn.augment(text)
            new_text.append(augmented_text)
    
    
    ## dataframe
    new=pd.DataFrame({'Description_normalized':new_text})
    df=shuffle(df.append(new).reset_index(drop=True))
    return df

df_n=df[df.Accident_Level!=1].reset_index(drop=True)

df_n["Accident_Level"].value_counts(normalize=True)

text = df_n.iloc[1]['Description_normalized']
augmented_text = aug_syn.augment(text)
print(text)
print(augmented_text)

df_n.head(5)

## data augmentation loop
new_text=[]
Accident_Level_new=[]
for i in tqdm(np.random.randint(0,len(df_n),355)):
  text = df_n.iloc[i]['Description_normalized']
  augmented_text = aug_syn.augment(text)
  new_text.append(augmented_text)
  Accident_Level_text=df_n.iloc[i]['Accident_Level']
  Accident_Level_new.append(Accident_Level_text)
  new=pd.DataFrame({'Description_normalized':new_text,'Accident_Level':Accident_Level_new})

new.head(5)

new.shape

new["Accident_Level"].value_counts(normalize=True)

df=shuffle(df.append(new).reset_index(drop=True))

df.shape

from sklearn.utils import shuffle

def augment_text(df,samples=300,pr=0.2):
    aug_syn.aug_p=pr
    new_text=[]
    Accident_Level_new=[]
    
    ##dropping samples from validation
    df_n=df[df.Accident_Level!=1].reset_index(drop=True)

    ## data augmentation loop
    for i in tqdm(np.random.randint(0,len(df_n),samples)):
      text = df_n.iloc[i]['Description_normalized']
      augmented_text = aug_syn.augment(text)
      new_text.append(augmented_text)
      Accident_Level_text=df_n.iloc[i]['Accident_Level']
      Accident_Level_new.append(Accident_Level_text)
    
      ## dataframe
      new=pd.DataFrame({'Description_normalized':new_text,'Accident_Level':Accident_Level_new})
      df=shuffle(df.append(new).reset_index(drop=True))
      return df

train = augment_text(train,samples=400)   ## change samples to 0 for no augmentation

train.shape

train.head()

train.isna().any()

train["Accident_Level"].value_counts(normalize=True)

sns.countplot(x = 'Accident_Level', data = train)

# Select input and output features
X_text = train['Description_normalized']
y_text = train['Accident_Level']

X_text

# Encode labels in column 'Accident Level'.
from sklearn.preprocessing import LabelEncoder
y_text = LabelEncoder().fit_transform(y_text)

# pre-processing methods
from sklearn.model_selection import train_test_split

# Divide our data into testing and training sets:
X_text_train, X_text_test, y_text_train, y_text_test = train_test_split(X_text, y_text, test_size = 0.20, random_state = 1, stratify = y_text)

print('X_text_train shape : ({0})'.format(X_text_train.shape[0]))
print('y_text_train shape : ({0},)'.format(y_text_train.shape[0]))
print('X_text_test shape : ({0})'.format(X_text_test.shape[0]))
print('y_text_test shape : ({0},)'.format(y_text_test.shape[0]))

from keras.utils import np_utils
# Convert both the training and test labels into one-hot encoded vectors:
# Convert both the training and test labels into one-hot encoded vectors:
y_text_train = np_utils.to_categorical(y_text_train)
y_text_test = np_utils.to_categorical(y_text_test)

# Keras pre-processing
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# The first step in word embeddings is to convert the words into thier corresponding numeric indexes.
tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(X_text_train)

X_text_train = tokenizer.texts_to_sequences(X_text_train)
X_text_test = tokenizer.texts_to_sequences(X_text_test)

# Sentences can have different lengths, and therefore the sequences returned by the Tokenizer class also consist of variable lengths.
# We need to pad the our sequences using the max length.
vocab_size = len(tokenizer.word_index) + 1
print("vocab_size:", vocab_size)

maxlen = 100

X_text_train = pad_sequences(X_text_train, padding='post', maxlen=maxlen)
X_text_test = pad_sequences(X_text_test, padding='post', maxlen=maxlen)

embedding_size = 50
embeddings_dictionary = dict()

glove_file = open('/content/drive/MyDrive/Colab Notebooks/glove.6B.50d (3).txt', encoding="utf8")

for line in glove_file:
    records = line.split()
    word = records[0]
    vector_dimensions = np.asarray(records[1:], dtype='float32')
    embeddings_dictionary[word] = vector_dimensions

glove_file.close()

embedding_matrix = np.zeros((vocab_size, embedding_size))

for word, index in tokenizer.word_index.items():
    embedding_vector = embeddings_dictionary.get(word)
    if embedding_vector is not None:
        embedding_matrix[index] = embedding_vector

len(embeddings_dictionary.values())

from keras.layers import Input
from tensorflow.keras.layers import Flatten, Activation, Dense, LSTM, BatchNormalization, Embedding, Dropout, Flatten, Bidirectional, GlobalMaxPool1D
from keras.models import Model
from tensorflow.keras.optimizers import SGD
# Build a LSTM Neural Network
deep_inputs = Input(shape=(maxlen,))
embedding_layer = Embedding(vocab_size, embedding_size, weights=[embedding_matrix], trainable=False)(deep_inputs)

LSTM_Layer_1 = Bidirectional(LSTM(128, return_sequences = True))(embedding_layer)
max_pool_layer_1 = GlobalMaxPool1D()(LSTM_Layer_1)
drop_out_layer_1 = Dropout(0.5, input_shape = (256,))(max_pool_layer_1)
dense_layer_1 = Dense(128, activation = 'relu')(drop_out_layer_1)
drop_out_layer_2 = Dropout(0.5, input_shape = (128,))(dense_layer_1)
dense_layer_2 = Dense(64, activation = 'relu')(drop_out_layer_2)
drop_out_layer_3 = Dropout(0.5, input_shape = (64,))(dense_layer_2)
dense_layer_3 = Dense(32, activation = 'relu')(drop_out_layer_3)
drop_out_layer_4 = Dropout(0.5, input_shape = (32,))(dense_layer_3)
dense_layer_4 = Dense(10, activation = 'relu')(drop_out_layer_4)
drop_out_layer_5 = Dropout(0.5, input_shape = (10,))(dense_layer_4)
dense_layer_5 = Dense(5, activation='softmax')(drop_out_layer_5)
model_lstm_text_input_DA = Model(inputs=deep_inputs, outputs=dense_layer_5)
opt = SGD(learning_rate=0.001, momentum=0.9)
model_lstm_text_input_DA.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['acc'])

print(model_lstm_text_input_DA.summary())

from keras.utils.vis_utils import plot_model
plot_model(model_lstm_text_input_DA, to_file='model_plot1.png', show_shapes=True, show_dtype=True, show_layer_names=True)

class Metrics(tf.keras.callbacks.Callback):

    def __init__(self, validation_data=()):
        super().__init__()
        self.validation_data = validation_data

    def on_train_begin(self, logs={}):
        self.val_f1s = []
        self.val_recalls = []
        self.val_precisions = []

    def on_epoch_end(self, epoch, logs={}):
        xVal, yVal, target_type = self.validation_data
        if target_type == 'multi_class':
          val_predict_classes = model.predict_classes(xVal, verbose=0) # Multiclass
        else:
          val_predict_classes = (np.asarray(self.model.predict(xVal))).round() # Multilabel
        
        
        val_targ = yVal

        _val_f1 = f1_score(val_targ, val_predict_classes, average='micro')
        _val_recall = recall_score(val_targ, val_predict_classes, average='micro')
        _val_precision = precision_score(val_targ, val_predict_classes, average='micro')
        self.val_f1s.append(_val_f1)
        self.val_recalls.append(_val_recall)
        self.val_precisions.append(_val_precision)
        #print("— train_f1: %f — train_precision: %f — train_recall %f" % (_val_f1, _val_precision, _val_recall))
        return

import tensorflow as tf
from keras.callbacks import ReduceLROnPlateau
from keras.callbacks import EarlyStopping
from sklearn import metrics
from sklearn import metrics
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, recall_score, precision_score, roc_auc_score
# Use earlystopping
# callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5, min_delta=0.001)
callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=7, min_delta=1E-3)
rlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.0001, patience=5, min_delta=1E-4)

target_type = 'multi_label'
metrics = Metrics(validation_data=(X_text_train, y_text_train, target_type))

# fit the keras model on the dataset
training_history = model_lstm_text_input_DA.fit(X_text_train, y_text_train, epochs=100, batch_size=8, verbose=1, validation_data=(X_text_test, y_text_test), callbacks=[rlrp, metrics])

# evaluate the keras model
_, train_accuracy = model_lstm_text_input_DA.evaluate(X_text_train, y_text_train, batch_size=8, verbose=0)
_, test_accuracy = model_lstm_text_input_DA.evaluate(X_text_test, y_text_test, batch_size=8, verbose=0)

print('Train accuracy: %.2f' % (train_accuracy*100))
print('Test accuracy: %.2f' % (test_accuracy*100))